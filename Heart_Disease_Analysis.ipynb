{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8afba0bf",
   "metadata": {},
   "source": [
    "# Heart Disease Data Analysis\n",
    "This notebook performs data cleaning, transformation, error correction, and model building on the heart disease dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d60f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff15d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "heart = pd.read_csv(\"heart.csv\")\n",
    "air = pd.read_csv(\"airquality.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73c5b9",
   "metadata": {},
   "source": [
    "## Step 1: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936de5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with mean (only numeric columns)\n",
    "heart = heart.fillna(heart.mean(numeric_only=True))\n",
    "air = air.fillna(air.mean(numeric_only=True))\n",
    "\n",
    "# Remove duplicate rows\n",
    "heart = heart.drop_duplicates()\n",
    "air = air.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4aaf76",
   "metadata": {},
   "source": [
    "## Step 2: Error Correcting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a82ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove wrong/invalid data (like zero or negative blood pressure)\n",
    "heart = heart[heart[\"trestbps\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af24ee67",
   "metadata": {},
   "source": [
    "## Step 3: Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd67a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data to keep original safe\n",
    "heart_scaled = heart.copy()\n",
    "\n",
    "# Select numeric columns to scale\n",
    "numeric_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "heart_scaled[numeric_cols] = scaler.fit_transform(heart_scaled[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24327006",
   "metadata": {},
   "source": [
    "## Step 4: Data Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b97761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and label\n",
    "X = heart_scaled.drop(\"target\", axis=1)\n",
    "y = heart_scaled[\"target\"]\n",
    "\n",
    "# Split into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and check accuracy\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Model Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
